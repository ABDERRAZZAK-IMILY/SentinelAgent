{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52faeae8",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "SentinelAgent ML Model Training - Google Colab Notebook\n",
    "========================================================\n",
    "\n",
    "This notebook trains a threat detection model and exports it for use in production.\n",
    "\n",
    "Run each cell sequentially by clicking the play button or pressing Shift+Enter.\n",
    "\"\"\"\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 1: Install Required Packages\n",
    "# =============================================================================\n",
    "print(\" Installing required packages...\")\n",
    "!pip install -q scikit-learn pandas numpy matplotlib seaborn joblib\n",
    "\n",
    "print(\" Packages installed successfully!\")\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 2: Import Libraries\n",
    "# =============================================================================\n",
    "print(\"Importing libraries...\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, \n",
    "                            accuracy_score, precision_recall_fscore_support,\n",
    "                            roc_curve, auc, roc_auc_score)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import joblib\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better visualizations\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\" Libraries imported successfully!\")\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 3: Generate Training Data\n",
    "# =============================================================================\n",
    "print(\" Generating training data...\")\n",
    "\n",
    "def generate_threat_data(n_samples=5000, random_state=42):\n",
    "    \"\"\"\n",
    "    Generate synthetic threat detection training data\n",
    "    \n",
    "    In production, replace this with your actual log data from:\n",
    "    - Network traffic logs\n",
    "    - System logs\n",
    "    - Security events\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Generate features\n",
    "    data = {\n",
    "        'packet_size': np.random.randint(64, 1500, n_samples),\n",
    "        'request_frequency': np.random.randint(1, 150, n_samples),\n",
    "        'port_number': np.random.choice([21, 22, 23, 80, 443, 3389, 8080, 8443], n_samples),\n",
    "        'failed_attempts': np.random.randint(0, 15, n_samples),\n",
    "        'connection_duration': np.random.randint(1, 7200, n_samples),\n",
    "        'payload_entropy': np.random.uniform(0, 8, n_samples),\n",
    "        'is_encrypted': np.random.choice([0, 1], n_samples, p=[0.3, 0.7]),\n",
    "        'geo_risk_score': np.random.uniform(0, 10, n_samples),\n",
    "        'unusual_port': np.random.choice([0, 1], n_samples, p=[0.85, 0.15]),\n",
    "        'time_of_day': np.random.randint(0, 24, n_samples),  # 0-23 hours\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Generate threat labels based on realistic rules\n",
    "    threat_level = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        score = 0\n",
    "        \n",
    "        # High risk indicators\n",
    "        if row['failed_attempts'] > 8:\n",
    "            score += 3\n",
    "        elif row['failed_attempts'] > 4:\n",
    "            score += 2\n",
    "            \n",
    "        if row['payload_entropy'] > 7.5:\n",
    "            score += 3\n",
    "        elif row['payload_entropy'] > 6.5:\n",
    "            score += 2\n",
    "            \n",
    "        if row['request_frequency'] > 100:\n",
    "            score += 3\n",
    "        elif row['request_frequency'] > 60:\n",
    "            score += 2\n",
    "            \n",
    "        if row['geo_risk_score'] > 8:\n",
    "            score += 2\n",
    "        elif row['geo_risk_score'] > 6:\n",
    "            score += 1\n",
    "            \n",
    "        if row['unusual_port'] == 1:\n",
    "            score += 2\n",
    "            \n",
    "        # Night time activity (suspicious)\n",
    "        if row['time_of_day'] >= 1 and row['time_of_day'] <= 5:\n",
    "            score += 1\n",
    "            \n",
    "        # Short connections with high frequency\n",
    "        if row['connection_duration'] < 30 and row['request_frequency'] > 50:\n",
    "            score += 2\n",
    "            \n",
    "        # Classification\n",
    "        if score >= 8:\n",
    "            threat_level.append(2)  # Malicious\n",
    "        elif score >= 4:\n",
    "            threat_level.append(1)  # Suspicious\n",
    "        else:\n",
    "            threat_level.append(0)  # Benign\n",
    "    \n",
    "    df['threat_level'] = threat_level\n",
    "    \n",
    "    # Add some noise to make it more realistic\n",
    "    noise_indices = np.random.choice(n_samples, size=int(n_samples * 0.05), replace=False)\n",
    "    for idx in noise_indices:\n",
    "        df.at[idx, 'threat_level'] = np.random.choice([0, 1, 2])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate dataset\n",
    "df = generate_threat_data(n_samples=5000)\n",
    "\n",
    "print(f\" Generated {len(df)} samples\")\n",
    "print(f\"\\n Dataset Shape: {df.shape}\")\n",
    "print(f\"\\n Target Distribution:\")\n",
    "print(df['threat_level'].value_counts().sort_index())\n",
    "print(f\"\\n   0 = Benign: {(df['threat_level'] == 0).sum()}\")\n",
    "print(f\"   1 = Suspicious: {(df['threat_level'] == 1).sum()}\")\n",
    "print(f\"   2 = Malicious: {(df['threat_level'] == 2).sum()}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\n Sample Data:\")\n",
    "df.head(10)\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 4: Exploratory Data Analysis (EDA)\n",
    "# =============================================================================\n",
    "print(\" Performing Exploratory Data Analysis...\")\n",
    "\n",
    "# Threat level distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Threat distribution\n",
    "threat_counts = df['threat_level'].value_counts().sort_index()\n",
    "axes[0, 0].bar(['Benign', 'Suspicious', 'Malicious'], threat_counts.values, \n",
    "               color=['green', 'orange', 'red'], alpha=0.7)\n",
    "axes[0, 0].set_title('Threat Level Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Count')\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 2. Failed attempts by threat level\n",
    "df.boxplot(column='failed_attempts', by='threat_level', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Failed Attempts by Threat Level', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Threat Level')\n",
    "axes[0, 1].set_ylabel('Failed Attempts')\n",
    "plt.sca(axes[0, 1])\n",
    "plt.xticks([1, 2, 3], ['Benign', 'Suspicious', 'Malicious'])\n",
    "\n",
    "# 3. Payload entropy by threat level\n",
    "df.boxplot(column='payload_entropy', by='threat_level', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Payload Entropy by Threat Level', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Threat Level')\n",
    "axes[1, 0].set_ylabel('Payload Entropy')\n",
    "plt.sca(axes[1, 0])\n",
    "plt.xticks([1, 2, 3], ['Benign', 'Suspicious', 'Malicious'])\n",
    "\n",
    "# 4. Request frequency by threat level\n",
    "df.boxplot(column='request_frequency', by='threat_level', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Request Frequency by Threat Level', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Threat Level')\n",
    "axes[1, 1].set_ylabel('Request Frequency')\n",
    "plt.sca(axes[1, 1])\n",
    "plt.xticks([1, 2, 3], ['Benign', 'Suspicious', 'Malicious'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation heatmap\n",
    "print(\"\\n Feature Correlation Heatmap:\")\n",
    "plt.figure(figsize=(12, 8))\n",
    "correlation_matrix = df.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            center=0, square=True, linewidths=1)\n",
    "plt.title('Feature Correlation Matrix', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 5: Prepare Data for Training\n",
    "# =============================================================================\n",
    "print(\" Preparing data for training...\")\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('threat_level', axis=1)\n",
    "y = df['threat_level']\n",
    "\n",
    "feature_names = X.columns.tolist()\n",
    "print(f\"Features: {feature_names}\")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\n Data Split:\")\n",
    "print(f\"   Training set: {len(X_train)} samples\")\n",
    "print(f\"   Test set: {len(X_test)} samples\")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\n Data preparation complete!\")\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 6: Train Multiple Models and Compare\n",
    "# =============================================================================\n",
    "print(\" Training multiple models for comparison...\")\n",
    "\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(max_depth=10, random_state=42)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {name}...\")\n",
    "    \n",
    "    # Train\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'predictions': y_pred\n",
    "    }\n",
    "    \n",
    "    print(f\" {name} trained successfully!\")\n",
    "    print(f\"   Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"   Precision: {precision:.4f}\")\n",
    "    print(f\"   Recall: {recall:.4f}\")\n",
    "    print(f\"   F1-Score: {f1:.4f}\")\n",
    "\n",
    "# Compare models\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\" MODEL COMPARISON\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'Accuracy': [r['accuracy'] for r in results.values()],\n",
    "    'Precision': [r['precision'] for r in results.values()],\n",
    "    'Recall': [r['recall'] for r in results.values()],\n",
    "    'F1-Score': [r['f1'] for r in results.values()]\n",
    "})\n",
    "\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Find best model\n",
    "best_model_name = max(results.items(), key=lambda x: x[1]['accuracy'])[0]\n",
    "best_model = results[best_model_name]['model']\n",
    "\n",
    "print(f\"\\n Best Model: {best_model_name}\")\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 7: Detailed Evaluation of Best Model\n",
    "# =============================================================================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"DETAILED EVALUATION: {best_model_name}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "y_pred_best = results[best_model_name]['predictions']\n",
    "\n",
    "# Classification report\n",
    "print(\"\\n Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_best, \n",
    "                          target_names=['Benign', 'Suspicious', 'Malicious']))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Confusion matrix heatmap\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=['Benign', 'Suspicious', 'Malicious'],\n",
    "            yticklabels=['Benign', 'Suspicious', 'Malicious'])\n",
    "axes[0].set_title(f'Confusion Matrix - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('True Label')\n",
    "axes[0].set_xlabel('Predicted Label')\n",
    "\n",
    "# Feature importance (for tree-based models)\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=True)\n",
    "    \n",
    "    axes[1].barh(feature_importance['feature'], feature_importance['importance'], \n",
    "                 color='steelblue', alpha=0.7)\n",
    "    axes[1].set_title('Feature Importance', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_xlabel('Importance')\n",
    "    axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 8: Hyperparameter Tuning (Optional but Recommended)\n",
    "# =============================================================================\n",
    "print(\"\\nüîß Performing hyperparameter tuning...\")\n",
    "\n",
    "if best_model_name == 'Random Forest':\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [10, 15, 20],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'min_samples_leaf': [1, 2]\n",
    "    }\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "        param_grid,\n",
    "        cv=3,\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\" Training with GridSearchCV (this may take a few minutes)...\")\n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    print(f\"\\n Best Parameters: {grid_search.best_params_}\")\n",
    "    print(f\" Best CV Score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Use best model\n",
    "    final_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Evaluate tuned model\n",
    "    y_pred_tuned = final_model.predict(X_test_scaled)\n",
    "    accuracy_tuned = accuracy_score(y_test, y_pred_tuned)\n",
    "    \n",
    "    print(f\"\\n Tuned Model Test Accuracy: {accuracy_tuned:.4f}\")\n",
    "    print(f\"   Improvement: {(accuracy_tuned - results[best_model_name]['accuracy']):.4f}\")\n",
    "else:\n",
    "    final_model = best_model\n",
    "    print(f\"Using {best_model_name} without additional tuning\")\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 9: Save Model and Artifacts\n",
    "# =============================================================================\n",
    "print(\"\\n Saving model and artifacts...\")\n",
    "\n",
    "# Create model directory\n",
    "import os\n",
    "model_dir = 'sentinel_model'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "model_path = f'{model_dir}/threat_model.pkl'\n",
    "joblib.dump(final_model, model_path)\n",
    "print(f\" Model saved to: {model_path}\")\n",
    "\n",
    "# Save scaler\n",
    "scaler_path = f'{model_dir}/scaler.pkl'\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\" Scaler saved to: {scaler_path}\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'model_type': best_model_name,\n",
    "    'feature_names': feature_names,\n",
    "    'trained_date': datetime.now().isoformat(),\n",
    "    'accuracy': float(accuracy_score(y_test, final_model.predict(X_test_scaled))),\n",
    "    'n_samples': len(df),\n",
    "    'train_test_split': 0.2,\n",
    "    'threat_levels': {\n",
    "        0: 'Benign',\n",
    "        1: 'Suspicious',\n",
    "        2: 'Malicious'\n",
    "    }\n",
    "}\n",
    "\n",
    "metadata_path = f'{model_dir}/metadata.json'\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(f\" Metadata saved to: {metadata_path}\")\n",
    "\n",
    "print(f\"\\n All files saved in '{model_dir}/' directory\")\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 10: Test the Saved Model\n",
    "# =============================================================================\n",
    "print(\"\\n Testing the saved model...\")\n",
    "\n",
    "# Load model\n",
    "loaded_model = joblib.load(model_path)\n",
    "loaded_scaler = joblib.load(scaler_path)\n",
    "\n",
    "# Test data\n",
    "test_samples = [\n",
    "    {\n",
    "        'packet_size': 1200,\n",
    "        'request_frequency': 95,\n",
    "        'port_number': 80,\n",
    "        'failed_attempts': 8,\n",
    "        'connection_duration': 250,\n",
    "        'payload_entropy': 7.8,\n",
    "        'is_encrypted': 0,\n",
    "        'geo_risk_score': 8.5,\n",
    "        'unusual_port': 0,\n",
    "        'time_of_day': 3\n",
    "    },\n",
    "    {\n",
    "        'packet_size': 850,\n",
    "        'request_frequency': 12,\n",
    "        'port_number': 443,\n",
    "        'failed_attempts': 1,\n",
    "        'connection_duration': 1200,\n",
    "        'payload_entropy': 4.2,\n",
    "        'is_encrypted': 1,\n",
    "        'geo_risk_score': 2.1,\n",
    "        'unusual_port': 0,\n",
    "        'time_of_day': 14\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"\\n Testing with sample data:\\n\")\n",
    "\n",
    "for i, sample in enumerate(test_samples, 1):\n",
    "    # Prepare data\n",
    "    sample_df = pd.DataFrame([sample])\n",
    "    sample_scaled = loaded_scaler.transform(sample_df)\n",
    "    \n",
    "    # Predict\n",
    "    prediction = loaded_model.predict(sample_scaled)[0]\n",
    "    probabilities = loaded_model.predict_proba(sample_scaled)[0]\n",
    "    \n",
    "    threat_labels = ['Benign', 'Suspicious', 'Malicious']\n",
    "    \n",
    "    print(f\"Sample {i}:\")\n",
    "    print(f\"  Failed Attempts: {sample['failed_attempts']}\")\n",
    "    print(f\"  Request Frequency: {sample['request_frequency']}\")\n",
    "    print(f\"  Payload Entropy: {sample['payload_entropy']:.2f}\")\n",
    "    print(f\"  ‚Üí Prediction: {threat_labels[prediction]} (confidence: {probabilities[prediction]:.2%})\")\n",
    "    print(f\"  ‚Üí Probabilities: Benign={probabilities[0]:.2%}, \"\n",
    "          f\"Suspicious={probabilities[1]:.2%}, Malicious={probabilities[2]:.2%}\\n\")\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 11: Download Model Files\n",
    "# =============================================================================\n",
    "print(\"\\n Preparing files for download...\")\n",
    "\n",
    "# Zip the model directory\n",
    "import shutil\n",
    "shutil.make_archive('sentinel_model', 'zip', model_dir)\n",
    "\n",
    "print(\"\\n Model package created: sentinel_model.zip\")\n",
    "print(\"\\n Package contents:\")\n",
    "print(\"   - threat_model.pkl (trained model)\")\n",
    "print(\"   - scaler.pkl (feature scaler)\")\n",
    "print(\"   - metadata.json (model information)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" TRAINING COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n Download 'sentinel_model.zip' from the Files panel (left sidebar)\")\n",
    "print(\" Extract the files and place them in your project's 'models/' directory\")\n",
    "print(\"\\n Ready to use in production!\")\n",
    "\n",
    "# Download instructions\n",
    "print(\"\\n To download:\")\n",
    "print(\"   1. Click the folder icon (üìÅ) in the left sidebar\")\n",
    "print(\"   2. Find 'sentinel_model.zip'\")\n",
    "print(\"   3. Right-click ‚Üí Download\")\n",
    "print(\"   4. Extract and use in your SentinelAgent project\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
